{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "74b58b71",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Torch version : 2.5.1\n",
      "CUDA dispo : True\n",
      "GPU : NVIDIA GeForce RTX 4060 Laptop GPU\n",
      "CPU ok : torch.Size([1000, 1000])\n",
      "GPU ok : torch.Size([1000, 1000])\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "print(\"Torch version :\", torch.__version__)\n",
    "print(\"CUDA dispo :\", torch.cuda.is_available())\n",
    "\n",
    "if torch.cuda.is_available():\n",
    "    print(\"GPU :\", torch.cuda.get_device_name(0))\n",
    "\n",
    "x = torch.rand(1000, 1000)\n",
    "y = torch.mm(x, x)\n",
    "\n",
    "print(\"CPU ok :\", y.shape)\n",
    "\n",
    "if torch.cuda.is_available():\n",
    "    x = x.to(\"cuda\")\n",
    "    y = torch.mm(x, x)\n",
    "    print(\"GPU ok :\", y.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "6abc7e6c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import time\n",
    "import numpy as np\n",
    "import csv\n",
    "from pathlib import Path\n",
    "import torch.nn as nn"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f2d534da",
   "metadata": {},
   "source": [
    "### Si tout marche bien coté installation/fonctionnement du matos on commence\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1db6d162",
   "metadata": {},
   "source": [
    "## Plan comparatif\n",
    "- Opérations de reference: matmul FP32 (CPU/GPU), meme tailles N. CNN simple pour faire une demo concret\n",
    "- Metriques: latence (mean/p50/p95), debit (GFLOPS), decomposition H2D/compute/D2H, energie estimee (POWER_W).\n",
    "- TPU: a faire sur Colab (torch_xla ou jax faut voir ce qui marche car j'ai pas encore pu des masses faire collab car bug)\n",
    "- Analyse: comparer par taille, et par energie/latence; discuter usages cibles (embarque, inference, HPC).\n",
    "\n",
    "## TODO (samedi)\n",
    "- Definir les operations de calcul de reference (matmul, CNN, ...)\n",
    "- Lister le nombre/type de processeurs a tester (CPU, GPU, TPU (collab mais faire gaffe car token réduit))\n",
    "- Chercher comment afficher les metriques recherchees (retour console, on stock dans un csv les valeurs puis on fait un graph de celui-ci)\n",
    "- Lancer l execution et recolter les donnees (fait)\n",
    "- Calculer les perfs voulues basees sur les resultats (plus définir l'utilisation optimal ? genre utiliser quoi dans quelle situation)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "368b0340",
   "metadata": {},
   "source": [
    "# Bench_matmul (CPU vs GPU)\n",
    "le TPU se fait sur collab"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "b8be518b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Infos: {'torch_version': '2.5.1', 'cpu_threads': 8, 'cuda_available': True, 'gpu_name': 'NVIDIA GeForce RTX 4060 Laptop GPU'}\n",
      "Debut des tests de multiplication matricielle\n",
      "\n",
      "\n",
      "On collecte: taille N, mean_ms, p50_ms, p95_ms, gflops.\n",
      "GPU: h2d_ms, compute_ms, d2h_ms pour decomposer le temps.\n",
      "Energie: power_w * temps (renseigner POWER_W).\n",
      "\n",
      "\n",
      "=== Taille S (N=1024) ===\n",
      "CPU: {'mean_ms': np.float64(8.323583994060755), 'p50_ms': np.float64(8.350899996003136), 'p95_ms': np.float64(9.748889959882945), 'gflops': np.float64(257.9998771601662), 'h2d_ms': 0, 'compute_ms': np.float64(8.323583994060755), 'd2h_ms': 0, 'energy_j': np.float64(0.37456127973273395), 'compute_energy_j': np.float64(0.37456127973273395)}\n",
      "GPU: {'mean_ms': np.float64(1.7944808692811056), 'p50_ms': np.float64(1.6639439759310335), 'p95_ms': np.float64(1.8210959578165784), 'gflops': np.float64(1196.715821696284), 'h2d_ms': 0.8217349951155484, 'compute_ms': 0.37193087846040723, 'd2h_ms': 0.6008149957051501, 'energy_j': np.float64(0.1615032782352995), 'compute_energy_j': 0.03347377906143665}\n",
      "\n",
      "=== Taille M (N=4096) ===\n",
      "CPU: {'mean_ms': np.float64(412.6285469980212), 'p50_ms': np.float64(410.7948500022758), 'p95_ms': np.float64(443.13382997061126), 'gflops': np.float64(333.0815438531913), 'h2d_ms': 0, 'compute_ms': np.float64(412.6285469980212), 'd2h_ms': 0, 'energy_j': np.float64(18.568284614910954), 'compute_energy_j': np.float64(18.568284614910954)}\n",
      "GPU: {'mean_ms': np.float64(57.93089119558688), 'p50_ms': np.float64(57.197332229232416), 'p95_ms': np.float64(65.08008681747596), 'gflops': np.float64(2372.4639934845327), 'h2d_ms': 20.340795000083745, 'compute_ms': 23.735340194702147, 'd2h_ms': 13.85475600080099, 'energy_j': np.float64(5.213780207602819), 'compute_energy_j': 2.1361806175231934}\n",
      "\n",
      "=== Taille L (N=8192) ===\n",
      "CPU: {'mean_ms': np.float64(3235.9423179994337), 'p50_ms': np.float64(3229.468250006903), 'p95_ms': np.float64(3387.976129972958), 'gflops': np.float64(339.78097250378505), 'h2d_ms': 0, 'compute_ms': np.float64(3235.9423179994337), 'd2h_ms': 0, 'energy_j': np.float64(145.61740430997452), 'compute_energy_j': np.float64(145.61740430997452)}\n",
      "GPU: {'mean_ms': np.float64(210.27996959670912), 'p50_ms': np.float64(209.41991956089623), 'p95_ms': np.float64(216.7985082173487), 'gflops': np.float64(5228.7986815136355), 'h2d_ms': 40.353200000245124, 'compute_ms': 114.36112159729004, 'd2h_ms': 55.56564799917396, 'energy_j': np.float64(18.92519726370382), 'compute_energy_j': 10.292500943756103}\n"
     ]
    }
   ],
   "source": [
    "# =========================\n",
    "# Configuration generale\n",
    "# =========================\n",
    "\n",
    "SIZES = { # tailles de matrices a tester\n",
    "    \"S\": 1024,\n",
    "    \"M\": 4096,\n",
    "    \"L\": 8192,  # ajuster selon votre RAM/GPU\n",
    "}\n",
    "\n",
    "WARMUP = 20  # iterations de warmup\n",
    "ITERS = 100\n",
    "DTYPE = torch.float32\n",
    "\n",
    "# Optionnel: forcer le nombre de threads CPU (ex: 1, 4, 8). None = auto.\n",
    "CPU_THREADS = None\n",
    "\n",
    "# Puissance moyenne en charge (W). Renseigner depuis la doc constructeur.\n",
    "# i7-11800H: 45W (TDP). RTX 3060 Laptop: 80W (TGP, confirme via nvidia-smi) (ici faut changer selon votre config les gars)\n",
    "POWER_W = {\n",
    "    \"cpu\": 45,      # AMD Ryzen 7 7735HS \n",
    "    \"gpu\": 90,      # RTX 4060 Laptop TGP \n",
    "}\n",
    "\n",
    "RESULTS_DIR = Path(\"results\")\n",
    "RESULTS_DIR.mkdir(exist_ok=True)\n",
    "CSV_PATH = RESULTS_DIR / \"matmul_cpu_gpu.csv\"\n",
    "\n",
    "if CPU_THREADS is not None:\n",
    "    torch.set_num_threads(CPU_THREADS)\n",
    "\n",
    "# =========================\n",
    "# Outils statistiques\n",
    "# =========================\n",
    "\n",
    "def compute_stats(times_ms):\n",
    "    times = np.array(times_ms)\n",
    "    return {\n",
    "        \"mean_ms\": times.mean(),\n",
    "        \"p50_ms\": np.percentile(times, 50),\n",
    "        \"p95_ms\": np.percentile(times, 95),\n",
    "    }\n",
    "\n",
    "\n",
    "def gflops(n, time_ms):\n",
    "    flops = 2 * (n ** 3)\n",
    "    return flops / (time_ms / 1000) / 1e9\n",
    "\n",
    "\n",
    "def energy_j(power_w, time_ms):\n",
    "    if power_w is None:\n",
    "        return np.nan\n",
    "    return power_w * (time_ms / 1000)\n",
    "\n",
    "\n",
    "def device_info():\n",
    "    info = {\n",
    "        \"torch_version\": torch.__version__,\n",
    "        \"cpu_threads\": torch.get_num_threads(),\n",
    "        \"cuda_available\": torch.cuda.is_available(),\n",
    "    }\n",
    "    if torch.cuda.is_available():\n",
    "        info[\"gpu_name\"] = torch.cuda.get_device_name(0)\n",
    "    return info\n",
    "\n",
    "\n",
    "# =========================\n",
    "# Benchmark CPU\n",
    "# =========================\n",
    "\n",
    "def bench_cpu(n):\n",
    "    x = torch.rand((n, n), dtype=DTYPE)\n",
    "    y = torch.rand((n, n), dtype=DTYPE)\n",
    "\n",
    "    # Warmup\n",
    "    for _ in range(WARMUP):\n",
    "        torch.mm(x, y)\n",
    "\n",
    "    times = []\n",
    "    for _ in range(ITERS):\n",
    "        start = time.perf_counter()\n",
    "        torch.mm(x, y)\n",
    "        end = time.perf_counter()\n",
    "        times.append((end - start) * 1000)\n",
    "\n",
    "    stats = compute_stats(times)\n",
    "    stats[\"gflops\"] = gflops(n, stats[\"mean_ms\"])\n",
    "    stats[\"h2d_ms\"] = 0\n",
    "    stats[\"compute_ms\"] = stats[\"mean_ms\"]\n",
    "    stats[\"d2h_ms\"] = 0\n",
    "    stats[\"energy_j\"] = energy_j(POWER_W[\"cpu\"], stats[\"mean_ms\"])\n",
    "    stats[\"compute_energy_j\"] = stats[\"energy_j\"]\n",
    "\n",
    "    return stats\n",
    "\n",
    "\n",
    "# =========================\n",
    "# Benchmark GPU\n",
    "# =========================\n",
    "\n",
    "def bench_gpu(n):\n",
    "    device = \"cuda\"\n",
    "\n",
    "    x_cpu = torch.rand((n, n), dtype=DTYPE, pin_memory=True)\n",
    "    y_cpu = torch.rand((n, n), dtype=DTYPE, pin_memory=True)\n",
    "\n",
    "    # Warmup complet\n",
    "    for _ in range(WARMUP):\n",
    "        x = x_cpu.to(device)\n",
    "        y = y_cpu.to(device)\n",
    "        torch.cuda.synchronize()\n",
    "        torch.mm(x, y)\n",
    "        torch.cuda.synchronize()\n",
    "\n",
    "    times = []\n",
    "    h2d_list = []\n",
    "    compute_list = []\n",
    "    d2h_list = []\n",
    "\n",
    "    for _ in range(ITERS):\n",
    "        # H -> D\n",
    "        t0 = time.perf_counter()\n",
    "        x = x_cpu.to(device)\n",
    "        y = y_cpu.to(device)\n",
    "        torch.cuda.synchronize()\n",
    "        h2d = (time.perf_counter() - t0) * 1000\n",
    "\n",
    "        # Compute (CUDA events)\n",
    "        start = torch.cuda.Event(enable_timing=True)\n",
    "        end = torch.cuda.Event(enable_timing=True)\n",
    "        start.record()\n",
    "        out = torch.mm(x, y)\n",
    "        end.record()\n",
    "        torch.cuda.synchronize()\n",
    "        compute = start.elapsed_time(end)\n",
    "\n",
    "        # D -> H\n",
    "        t0 = time.perf_counter()\n",
    "        out.cpu()\n",
    "        torch.cuda.synchronize()\n",
    "        d2h = (time.perf_counter() - t0) * 1000\n",
    "\n",
    "        total = h2d + compute + d2h\n",
    "        times.append(total)\n",
    "        h2d_list.append(h2d)\n",
    "        compute_list.append(compute)\n",
    "        d2h_list.append(d2h)\n",
    "\n",
    "    stats = compute_stats(times)\n",
    "    stats[\"gflops\"] = gflops(n, stats[\"mean_ms\"])\n",
    "    stats[\"h2d_ms\"] = float(np.mean(h2d_list))\n",
    "    stats[\"compute_ms\"] = float(np.mean(compute_list))\n",
    "    stats[\"d2h_ms\"] = float(np.mean(d2h_list))\n",
    "    stats[\"energy_j\"] = energy_j(POWER_W[\"gpu\"], stats[\"mean_ms\"])\n",
    "    stats[\"compute_energy_j\"] = energy_j(POWER_W[\"gpu\"], stats[\"compute_ms\"])\n",
    "\n",
    "    return stats\n",
    "\n",
    "\n",
    "# =========================\n",
    "# Sauvegarde CSV\n",
    "# =========================\n",
    "\n",
    "def save_row(device, size_tag, n, stats):\n",
    "    file_exists = CSV_PATH.exists()\n",
    "    with open(CSV_PATH, \"a\", newline=\"\") as f:\n",
    "        writer = csv.writer(f)\n",
    "        if not file_exists:\n",
    "            writer.writerow([\n",
    "                \"device\", \"size\", \"N\",\n",
    "                \"mean_ms\", \"p50_ms\", \"p95_ms\",\n",
    "                \"gflops\",\n",
    "                \"h2d_ms\", \"compute_ms\", \"d2h_ms\",\n",
    "                \"power_w\", \"energy_j\", \"compute_energy_j\",\n",
    "            ])\n",
    "\n",
    "        power_w = POWER_W[device]\n",
    "        writer.writerow([\n",
    "            device, size_tag, n,\n",
    "            stats[\"mean_ms\"], stats[\"p50_ms\"], stats[\"p95_ms\"],\n",
    "            stats[\"gflops\"],\n",
    "            stats[\"h2d_ms\"], stats[\"compute_ms\"], stats[\"d2h_ms\"],\n",
    "            power_w, stats[\"energy_j\"], stats[\"compute_energy_j\"],\n",
    "        ])\n",
    "\n",
    "\n",
    "# =========================\n",
    "# Execution principale\n",
    "# =========================\n",
    "\n",
    "def main():\n",
    "    info = device_info()\n",
    "    print(\"Infos:\", info)\n",
    "    print(\"Debut des tests de multiplication matricielle\\n\")\n",
    "    print(\"\\nOn collecte: taille N, mean_ms, p50_ms, p95_ms, gflops.\")\n",
    "    print(\"GPU: h2d_ms, compute_ms, d2h_ms pour decomposer le temps.\")\n",
    "    print(\"Energie: power_w * temps (renseigner POWER_W).\\n\")\n",
    "\n",
    "    for size_tag, n in SIZES.items():\n",
    "        print(f\"\\n=== Taille {size_tag} (N={n}) ===\")\n",
    "\n",
    "        # CPU\n",
    "        cpu_stats = bench_cpu(n)\n",
    "        print(\"CPU:\", cpu_stats)\n",
    "        save_row(\"cpu\", size_tag, n, cpu_stats)\n",
    "\n",
    "        # GPU\n",
    "        if torch.cuda.is_available():\n",
    "            gpu_stats = bench_gpu(n)\n",
    "            print(\"GPU:\", gpu_stats)\n",
    "            save_row(\"gpu\", size_tag, n, gpu_stats)\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7ff05529",
   "metadata": {},
   "source": [
    "Les résultats du test MatMul montrent que le GPU (RTX 3060) n’apporte qu’un gain limité pour les petites matrices (S) en raison du coût des transferts mémoire et de la sous-utilisation de celui-ci.\n",
    "\n",
    "\n",
    "En revanche, pour des tailles moyennes et grandes (M et L), le GPU atteint un débit d’environ 3,2 TFLOPS, soit un facteur d’accélération proche de ×6 par rapport au CPU.\n",
    "\n",
    "\n",
    "Résultats principaux\n",
    "\n",
    "Petites matrices (1024) :\n",
    "le GPU est seulement légèrement plus rapide, car les transferts mémoire dominent.\n",
    "--> Le CPU reste pertinent pour de petites charges.\n",
    "\n",
    "Matrices moyennes (4096) :\n",
    "le GPU devient ≈ 4× plus rapide.\n",
    "--> Le parallélisme GPU commence à être pleinement exploité.\n",
    "\n",
    "Grandes matrices (8192) :\n",
    "le GPU atteint --> 10× la performance du CPU tout en consommant beaucoup moins d’énergie,\n",
    "même si ~40 % du temps reste lié aux transferts mémoire.\n",
    "\n",
    "\n",
    "### Points clés à retenir\n",
    "\n",
    "Le CPU est adapté aux petites tâches ou aux traitements séquentiels.\n",
    "\n",
    "Le GPU devient dominant lorsque l’intensité de calcul augmente.\n",
    "\n",
    "Les transferts mémoire constituent un goulot d’étranglement réel.\n",
    "\n",
    "Les workloads d’IA massifs privilégient donc les GPU ou accélérateurs spécialisés."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4f888bb8",
   "metadata": {},
   "source": [
    "# Bench_CNN (CPU vs GPU)\n",
    "le TPU se fait sur collab"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "25f01f2e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== Benchmark CNN (CPU vs GPU) ===\n",
      "\n",
      "CPU: {'mean_ms': 10.735109996749088, 'p50_ms': 10.59424999402836, 'p95_ms': 12.288809986785052, 'energy_j': 0.48307994985370895}\n",
      "GPU: {'mean_ms': 2.8308799337409436, 'p50_ms': 3.1825219865422696, 'p95_ms': 4.012972954777069, 'h2d_ms': 0.7961340039037168, 'compute_ms': 2.034745929837227, 'energy_j': 0.2547791940366849}\n"
     ]
    }
   ],
   "source": [
    "# =========================\n",
    "# Configuration\n",
    "# =========================\n",
    "\n",
    "BATCH_SIZE = 32\n",
    "IMAGE_SIZE = 64\n",
    "WARMUP = 20\n",
    "ITERS = 100\n",
    "DTYPE = torch.float32\n",
    "\n",
    "POWER_W = {\n",
    "    \"cpu\": 45,   # AMD Ryzen 7 7735HS\n",
    "    \"gpu\": 90,   # RTX 4060 Laptop \n",
    "}\n",
    "\n",
    "RESULTS_DIR = Path(\"results\")\n",
    "RESULTS_DIR.mkdir(exist_ok=True)\n",
    "CSV_PATH = RESULTS_DIR / \"cnn_cpu_gpu.csv\"\n",
    "\n",
    "\n",
    "# =========================\n",
    "# Modèle CNN simple\n",
    "# =========================\n",
    "\n",
    "class SimpleCNN(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "\n",
    "        self.features = nn.Sequential(\n",
    "            nn.Conv2d(3, 16, kernel_size=3, padding=1),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(2),\n",
    "\n",
    "            nn.Conv2d(16, 32, kernel_size=3, padding=1),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(2),\n",
    "        )\n",
    "\n",
    "        self.classifier = nn.Sequential(\n",
    "            nn.Flatten(),\n",
    "            nn.Linear(32 * (IMAGE_SIZE // 4) * (IMAGE_SIZE // 4), 128),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(128, 10),\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.features(x)\n",
    "        x = self.classifier(x)\n",
    "        return x\n",
    "\n",
    "\n",
    "# =========================\n",
    "# Outils statistiques\n",
    "# =========================\n",
    "\n",
    "def compute_stats(times_ms):\n",
    "    t = np.array(times_ms)\n",
    "    return {\n",
    "        \"mean_ms\": float(t.mean()),\n",
    "        \"p50_ms\": float(np.percentile(t, 50)),\n",
    "        \"p95_ms\": float(np.percentile(t, 95)),\n",
    "    }\n",
    "\n",
    "\n",
    "def energy_j(power_w, time_ms):\n",
    "    return power_w * (time_ms / 1000)\n",
    "\n",
    "\n",
    "# =========================\n",
    "# Benchmark CPU\n",
    "# =========================\n",
    "\n",
    "def bench_cpu():\n",
    "    device = \"cpu\"\n",
    "\n",
    "    model = SimpleCNN().to(device)\n",
    "    model.eval()\n",
    "\n",
    "    x = torch.rand((BATCH_SIZE, 3, IMAGE_SIZE, IMAGE_SIZE), dtype=DTYPE)\n",
    "\n",
    "    # Warmup\n",
    "    with torch.no_grad():\n",
    "        for _ in range(WARMUP):\n",
    "            model(x)\n",
    "\n",
    "    times = []\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for _ in range(ITERS):\n",
    "            t0 = time.perf_counter()\n",
    "            model(x)\n",
    "            t1 = time.perf_counter()\n",
    "            times.append((t1 - t0) * 1000)\n",
    "\n",
    "    stats = compute_stats(times)\n",
    "    stats[\"energy_j\"] = energy_j(POWER_W[\"cpu\"], stats[\"mean_ms\"])\n",
    "    return stats\n",
    "\n",
    "\n",
    "# =========================\n",
    "# Benchmark GPU\n",
    "# =========================\n",
    "\n",
    "def bench_gpu():\n",
    "    device = \"cuda\"\n",
    "\n",
    "    model = SimpleCNN().to(device)\n",
    "    model.eval()\n",
    "\n",
    "    x_cpu = torch.rand((BATCH_SIZE, 3, IMAGE_SIZE, IMAGE_SIZE), dtype=DTYPE, pin_memory=True)\n",
    "\n",
    "    # Warmup\n",
    "    with torch.no_grad():\n",
    "        for _ in range(WARMUP):\n",
    "            x = x_cpu.to(device, non_blocking=True)\n",
    "            torch.cuda.synchronize()\n",
    "            model(x)\n",
    "            torch.cuda.synchronize()\n",
    "\n",
    "    times = []\n",
    "    h2d_list, compute_list = [], []\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for _ in range(ITERS):\n",
    "\n",
    "            # H2D\n",
    "            t0 = time.perf_counter()\n",
    "            x = x_cpu.to(device, non_blocking=True)\n",
    "            torch.cuda.synchronize()\n",
    "            h2d = (time.perf_counter() - t0) * 1000\n",
    "\n",
    "            # Compute\n",
    "            start = torch.cuda.Event(enable_timing=True)\n",
    "            end = torch.cuda.Event(enable_timing=True)\n",
    "\n",
    "            start.record()\n",
    "            model(x)\n",
    "            end.record()\n",
    "\n",
    "            torch.cuda.synchronize()\n",
    "            compute = start.elapsed_time(end)\n",
    "\n",
    "            times.append(h2d + compute)\n",
    "            h2d_list.append(h2d)\n",
    "            compute_list.append(compute)\n",
    "\n",
    "    stats = compute_stats(times)\n",
    "    stats[\"h2d_ms\"] = float(np.mean(h2d_list))\n",
    "    stats[\"compute_ms\"] = float(np.mean(compute_list))\n",
    "    stats[\"energy_j\"] = energy_j(POWER_W[\"gpu\"], stats[\"mean_ms\"])\n",
    "    return stats\n",
    "\n",
    "\n",
    "# =========================\n",
    "# Sauvegarde CSV\n",
    "# =========================\n",
    "\n",
    "def save_row(device, stats):\n",
    "    file_exists = CSV_PATH.exists()\n",
    "\n",
    "    with open(CSV_PATH, \"a\", newline=\"\") as f:\n",
    "        writer = csv.writer(f)\n",
    "\n",
    "        if not file_exists:\n",
    "            writer.writerow([\n",
    "                \"device\",\n",
    "                \"mean_ms\", \"p50_ms\", \"p95_ms\",\n",
    "                \"h2d_ms\", \"compute_ms\",\n",
    "                \"energy_j\",\n",
    "            ])\n",
    "\n",
    "        writer.writerow([\n",
    "            device,\n",
    "            stats[\"mean_ms\"], stats[\"p50_ms\"], stats[\"p95_ms\"],\n",
    "            stats.get(\"h2d_ms\", 0),\n",
    "            stats.get(\"compute_ms\", stats[\"mean_ms\"]),\n",
    "            stats[\"energy_j\"],\n",
    "        ])\n",
    "\n",
    "\n",
    "# =========================\n",
    "# Exécution\n",
    "# =========================\n",
    "\n",
    "def main():\n",
    "    print(\"=== Benchmark CNN (CPU vs GPU) ===\\n\")\n",
    "\n",
    "    cpu_stats = bench_cpu()\n",
    "    print(\"CPU:\", cpu_stats)\n",
    "    save_row(\"cpu\", cpu_stats)\n",
    "\n",
    "    if torch.cuda.is_available():\n",
    "        gpu_stats = bench_gpu()\n",
    "        print(\"GPU:\", gpu_stats)\n",
    "        save_row(\"gpu\", gpu_stats)\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0dd40334",
   "metadata": {},
   "source": [
    "Bon, même si le CNN utilisé ici reste volontairement simple, on voit déjà très clairement que le GPU devient beaucoup plus intéressant dès que la charge de calcul augmente.\n",
    "On passe d’environ 6,9 ms par inférence sur CPU à 0,7 ms sur GPU, soit presque 10× plus rapide, ce qui confirme que les réseaux convolutionnels exploitent très bien le parallélisme massif des GPU\n",
    "\n",
    "le CNN testé prend en entrée des images RGB de 64×64 avec un batch size de 32.\n",
    "L’architecture est composée de deux blocs convolutionnels (convolution + ReLU + max-pooling) \n",
    "Le benchmark est réalisé avec un warm-up comme pour le matmul pour stabiliser les performances puis plusieurs itérations pour mesurer une latence moyenne fiable,\n",
    "Côté GPU, on sépare aussi le temps de transfert mémoire et le temps de calcul réel.\n",
    "\n",
    "On voit pour commencer que le temps de transfert vers le GPU existe toujours, mais qu’il reste relativement faible par rapport au gain énorme en temps de calcul.\n",
    "On observe aussi que le GPU consomme moins d’énergie par inférence que le CPU, simplement parce qu’il termine le calcul beaucoup plus vite."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "b1285aad",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== CPU vs GPU Stress Benchmark ===\n",
      "\n",
      "=== Running on CPU ===\n",
      "Mean latency: 14513.89 ms/batch\n",
      "P95 latency:  15176.51 ms\n",
      "Throughput:   18 images/sec\n",
      "\n",
      "=== Running on CUDA ===\n",
      "Mean latency: 669.14 ms/batch\n",
      "P95 latency:  674.82 ms\n",
      "Throughput:   383 images/sec\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torchvision.models as models\n",
    "import numpy as np\n",
    "import time\n",
    "\n",
    "# =========================\n",
    "# CONFIG — adjust to stress hardware\n",
    "# =========================\n",
    "\n",
    "BATCH_SIZE = 256\n",
    "IMAGE_SIZE = 224\n",
    "WARMUP = 10\n",
    "ITERS = 50\n",
    "DTYPE = torch.float32\n",
    "\n",
    "# =========================\n",
    "# Benchmark function\n",
    "# =========================\n",
    "\n",
    "def run_benchmark(device):\n",
    "\n",
    "    print(f\"\\n=== Running on {device.upper()} ===\")\n",
    "\n",
    "    model = models.resnet50(weights=None).to(device)\n",
    "    model.eval()\n",
    "\n",
    "    x_cpu = torch.rand(\n",
    "        (BATCH_SIZE, 3, IMAGE_SIZE, IMAGE_SIZE),\n",
    "        dtype=DTYPE\n",
    "    )\n",
    "\n",
    "    if device == \"cuda\":\n",
    "        x = x_cpu.pin_memory().to(device, non_blocking=True)\n",
    "    else:\n",
    "        x = x_cpu\n",
    "\n",
    "    # Warmup\n",
    "    with torch.no_grad():\n",
    "        for _ in range(WARMUP):\n",
    "            out = model(x)\n",
    "            if device == \"cuda\":\n",
    "                torch.cuda.synchronize()\n",
    "\n",
    "    # Benchmark\n",
    "    times = []\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for _ in range(ITERS):\n",
    "\n",
    "            start = time.perf_counter()\n",
    "\n",
    "            out = model(x)\n",
    "\n",
    "            if device == \"cuda\":\n",
    "                torch.cuda.synchronize()\n",
    "\n",
    "            end = time.perf_counter()\n",
    "            times.append(end - start)\n",
    "\n",
    "    times = np.array(times)\n",
    "\n",
    "    latency_ms = times.mean() * 1000\n",
    "    throughput = BATCH_SIZE / times.mean()\n",
    "\n",
    "    print(f\"Mean latency: {latency_ms:.2f} ms/batch\")\n",
    "    print(f\"P95 latency:  {np.percentile(times,95)*1000:.2f} ms\")\n",
    "    print(f\"Throughput:   {throughput:.0f} images/sec\")\n",
    "\n",
    "# =========================\n",
    "# Main\n",
    "# =========================\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "\n",
    "    print(\"=== CPU vs GPU Stress Benchmark ===\")\n",
    "\n",
    "    run_benchmark(\"cpu\")\n",
    "\n",
    "    if torch.cuda.is_available():\n",
    "        run_benchmark(\"cuda\")\n",
    "    else:\n",
    "        print(\"\\n⚠ GPU not detected\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d61ce874",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "27cdca4a",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "torch_gpu",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
