{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "74b58b71",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Torch version : 2.5.1+cu121\n",
      "CUDA dispo : True\n",
      "GPU : NVIDIA GeForce RTX 3060 Laptop GPU\n",
      "CPU ok : torch.Size([1000, 1000])\n",
      "GPU ok : torch.Size([1000, 1000])\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "print(\"Torch version :\", torch.__version__)\n",
    "print(\"CUDA dispo :\", torch.cuda.is_available())\n",
    "\n",
    "if torch.cuda.is_available():\n",
    "    print(\"GPU :\", torch.cuda.get_device_name(0))\n",
    "\n",
    "x = torch.rand(1000, 1000)\n",
    "y = torch.mm(x, x)\n",
    "\n",
    "print(\"CPU ok :\", y.shape)\n",
    "\n",
    "if torch.cuda.is_available():\n",
    "    x = x.to(\"cuda\")\n",
    "    y = torch.mm(x, x)\n",
    "    print(\"GPU ok :\", y.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "6abc7e6c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import time\n",
    "import numpy as np\n",
    "import csv\n",
    "from pathlib import Path"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f2d534da",
   "metadata": {},
   "source": [
    "### Si tout marche bien coté installation/fonctionnement du matos on commence\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "368b0340",
   "metadata": {},
   "source": [
    "# Bench_matmul (CPU vs GPU)\n",
    "le TPU se fait sur collab"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1db6d162",
   "metadata": {},
   "source": [
    "## Plan comparatif\n",
    "- Ops de reference: matmul FP32 (CPU/GPU), meme tailles N.\n",
    "- Metriques: latence (mean/p50/p95), debit (GFLOPS), decomposition H2D/compute/D2H, energie estimee (POWER_W).\n",
    "- TPU: a faire sur Colab (torch_xla ou jax). Exporter un CSV avec les memes colonnes.\n",
    "- Analyse: comparer par taille, et par energie/latence; discuter usages cibles (embarque, inference, HPC).\n",
    "\n",
    "## TODO (samedi)\n",
    "- Definir les operations de calcul de reference\n",
    "- Lister le nombre/type de processeurs a tester\n",
    "- Chercher comment afficher les metriques recherchees\n",
    "- Lancer l execution et recolter les donnees\n",
    "- Calculer les perfs voulues basees sur les resultats"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "b8be518b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Infos: {'torch_version': '2.5.1+cu121', 'cpu_threads': 8, 'cuda_available': True, 'gpu_name': 'NVIDIA GeForce RTX 3060 Laptop GPU'}\n",
      "Debut des tests de multiplication matricielle\n",
      "\n",
      "\n",
      "On collecte: taille N, mean_ms, p50_ms, p95_ms, gflops.\n",
      "GPU: h2d_ms, compute_ms, d2h_ms pour decomposer le temps.\n",
      "Energie: power_w * temps (renseigner POWER_W).\n",
      "\n",
      "\n",
      "=== Taille S (N=1024) ===\n",
      "CPU: {'mean_ms': 10.805466000019806, 'p50_ms': 11.014000003342517, 'p95_ms': 11.836015001972555, 'gflops': 198.7404937460415, 'h2d_ms': 0, 'compute_ms': 10.805466000019806, 'd2h_ms': 0, 'energy_j': 0.4862459700008912, 'compute_energy_j': 0.4862459700008912}\n",
      "GPU: {'mean_ms': 7.073579716985114, 'p50_ms': 6.625951990747126, 'p95_ms': 9.120495578640838, 'gflops': 303.59220280552603, 'h2d_ms': 3.1194089998462005, 'compute_ms': 2.340758717060089, 'd2h_ms': 1.6134120000788243, 'energy_j': 0.5658863773588091, 'compute_energy_j': 0.18726069736480716}\n",
      "\n",
      "=== Taille M (N=4096) ===\n",
      "CPU: {'mean_ms': 604.3914229998336, 'p50_ms': 602.6006499996583, 'p95_ms': 662.6739500043186, 'gflops': 227.40056897206802, 'h2d_ms': 0, 'compute_ms': 604.3914229998336, 'd2h_ms': 0, 'energy_j': 27.19761403499251, 'compute_energy_j': 27.19761403499251}\n",
      "GPU: {'mean_ms': 145.9679559224163, 'p50_ms': 146.1923569945211, 'p95_ms': 160.02841706487814, 'gflops': 941.5693506391939, 'h2d_ms': 44.28055399999721, 'compute_ms': 76.22911392211914, 'd2h_ms': 25.458288000299945, 'energy_j': 11.677436473793305, 'compute_energy_j': 6.098329113769532}\n",
      "\n",
      "=== Taille L (N=8192) ===\n",
      "CPU: {'mean_ms': 3273.342835999865, 'p50_ms': 3303.9811000016925, 'p95_ms': 3591.0119049942296, 'gflops': 335.89870748755425, 'h2d_ms': 0, 'compute_ms': 3273.342835999865, 'd2h_ms': 0, 'energy_j': 147.30042761999394, 'compute_energy_j': 147.30042761999394}\n",
      "GPU: {'mean_ms': 274.37974912863166, 'p50_ms': 273.5366411519353, 'p95_ms': 284.62252534905565, 'gflops': 4007.2623116968416, 'h2d_ms': 56.08942599974398, 'compute_ms': 160.74730712890624, 'd2h_ms': 57.543015999981435, 'energy_j': 21.950379930290534, 'compute_energy_j': 12.8597845703125}\n"
     ]
    }
   ],
   "source": [
    "# =========================\n",
    "# Configuration generale\n",
    "# =========================\n",
    "\n",
    "SIZES = { # tailles de matrices a tester\n",
    "    \"S\": 1024,\n",
    "    \"M\": 4096,\n",
    "    \"L\": 8192,  # ajuster selon votre RAM/GPU\n",
    "}\n",
    "\n",
    "WARMUP = 20  # iterations de warmup\n",
    "ITERS = 100\n",
    "DTYPE = torch.float32\n",
    "\n",
    "# Optionnel: forcer le nombre de threads CPU (ex: 1, 4, 8). None = auto.\n",
    "CPU_THREADS = None\n",
    "\n",
    "# Puissance moyenne en charge (W). Renseigner depuis la doc constructeur.\n",
    "# i7-11800H: 45W (TDP). RTX 3060 Laptop: 80W (TGP, confirme via nvidia-smi) (ici faut changer selon votre config les gars)\n",
    "POWER_W = {\n",
    "    \"cpu\": 45,      # i7-11800H TDP\n",
    "    \"gpu\": 80,      # RTX 3060 Laptop TGP \n",
    "}\n",
    "\n",
    "RESULTS_DIR = Path(\"results\")\n",
    "RESULTS_DIR.mkdir(exist_ok=True)\n",
    "CSV_PATH = RESULTS_DIR / \"matmul_cpu_gpu.csv\"\n",
    "\n",
    "if CPU_THREADS is not None:\n",
    "    torch.set_num_threads(CPU_THREADS)\n",
    "\n",
    "# =========================\n",
    "# Outils statistiques\n",
    "# =========================\n",
    "\n",
    "def compute_stats(times_ms):\n",
    "    times = np.array(times_ms)\n",
    "    return {\n",
    "        \"mean_ms\": times.mean(),\n",
    "        \"p50_ms\": np.percentile(times, 50),\n",
    "        \"p95_ms\": np.percentile(times, 95),\n",
    "    }\n",
    "\n",
    "\n",
    "def gflops(n, time_ms):\n",
    "    flops = 2 * (n ** 3)\n",
    "    return flops / (time_ms / 1000) / 1e9\n",
    "\n",
    "\n",
    "def energy_j(power_w, time_ms):\n",
    "    if power_w is None:\n",
    "        return np.nan\n",
    "    return power_w * (time_ms / 1000)\n",
    "\n",
    "\n",
    "def device_info():\n",
    "    info = {\n",
    "        \"torch_version\": torch.__version__,\n",
    "        \"cpu_threads\": torch.get_num_threads(),\n",
    "        \"cuda_available\": torch.cuda.is_available(),\n",
    "    }\n",
    "    if torch.cuda.is_available():\n",
    "        info[\"gpu_name\"] = torch.cuda.get_device_name(0)\n",
    "    return info\n",
    "\n",
    "\n",
    "# =========================\n",
    "# Benchmark CPU\n",
    "# =========================\n",
    "\n",
    "def bench_cpu(n):\n",
    "    x = torch.rand((n, n), dtype=DTYPE)\n",
    "    y = torch.rand((n, n), dtype=DTYPE)\n",
    "\n",
    "    # Warmup\n",
    "    for _ in range(WARMUP):\n",
    "        torch.mm(x, y)\n",
    "\n",
    "    times = []\n",
    "    for _ in range(ITERS):\n",
    "        start = time.perf_counter()\n",
    "        torch.mm(x, y)\n",
    "        end = time.perf_counter()\n",
    "        times.append((end - start) * 1000)\n",
    "\n",
    "    stats = compute_stats(times)\n",
    "    stats[\"gflops\"] = gflops(n, stats[\"mean_ms\"])\n",
    "    stats[\"h2d_ms\"] = 0\n",
    "    stats[\"compute_ms\"] = stats[\"mean_ms\"]\n",
    "    stats[\"d2h_ms\"] = 0\n",
    "    stats[\"energy_j\"] = energy_j(POWER_W[\"cpu\"], stats[\"mean_ms\"])\n",
    "    stats[\"compute_energy_j\"] = stats[\"energy_j\"]\n",
    "\n",
    "    return stats\n",
    "\n",
    "\n",
    "# =========================\n",
    "# Benchmark GPU\n",
    "# =========================\n",
    "\n",
    "def bench_gpu(n):\n",
    "    device = \"cuda\"\n",
    "\n",
    "    x_cpu = torch.rand((n, n), dtype=DTYPE, pin_memory=True)\n",
    "    y_cpu = torch.rand((n, n), dtype=DTYPE, pin_memory=True)\n",
    "\n",
    "    # Warmup complet\n",
    "    for _ in range(WARMUP):\n",
    "        x = x_cpu.to(device)\n",
    "        y = y_cpu.to(device)\n",
    "        torch.cuda.synchronize()\n",
    "        torch.mm(x, y)\n",
    "        torch.cuda.synchronize()\n",
    "\n",
    "    times = []\n",
    "    h2d_list = []\n",
    "    compute_list = []\n",
    "    d2h_list = []\n",
    "\n",
    "    for _ in range(ITERS):\n",
    "        # H -> D\n",
    "        t0 = time.perf_counter()\n",
    "        x = x_cpu.to(device)\n",
    "        y = y_cpu.to(device)\n",
    "        torch.cuda.synchronize()\n",
    "        h2d = (time.perf_counter() - t0) * 1000\n",
    "\n",
    "        # Compute (CUDA events)\n",
    "        start = torch.cuda.Event(enable_timing=True)\n",
    "        end = torch.cuda.Event(enable_timing=True)\n",
    "        start.record()\n",
    "        out = torch.mm(x, y)\n",
    "        end.record()\n",
    "        torch.cuda.synchronize()\n",
    "        compute = start.elapsed_time(end)\n",
    "\n",
    "        # D -> H\n",
    "        t0 = time.perf_counter()\n",
    "        out.cpu()\n",
    "        torch.cuda.synchronize()\n",
    "        d2h = (time.perf_counter() - t0) * 1000\n",
    "\n",
    "        total = h2d + compute + d2h\n",
    "        times.append(total)\n",
    "        h2d_list.append(h2d)\n",
    "        compute_list.append(compute)\n",
    "        d2h_list.append(d2h)\n",
    "\n",
    "    stats = compute_stats(times)\n",
    "    stats[\"gflops\"] = gflops(n, stats[\"mean_ms\"])\n",
    "    stats[\"h2d_ms\"] = float(np.mean(h2d_list))\n",
    "    stats[\"compute_ms\"] = float(np.mean(compute_list))\n",
    "    stats[\"d2h_ms\"] = float(np.mean(d2h_list))\n",
    "    stats[\"energy_j\"] = energy_j(POWER_W[\"gpu\"], stats[\"mean_ms\"])\n",
    "    stats[\"compute_energy_j\"] = energy_j(POWER_W[\"gpu\"], stats[\"compute_ms\"])\n",
    "\n",
    "    return stats\n",
    "\n",
    "\n",
    "# =========================\n",
    "# Sauvegarde CSV\n",
    "# =========================\n",
    "\n",
    "def save_row(device, size_tag, n, stats):\n",
    "    file_exists = CSV_PATH.exists()\n",
    "    with open(CSV_PATH, \"a\", newline=\"\") as f:\n",
    "        writer = csv.writer(f)\n",
    "        if not file_exists:\n",
    "            writer.writerow([\n",
    "                \"device\", \"size\", \"N\",\n",
    "                \"mean_ms\", \"p50_ms\", \"p95_ms\",\n",
    "                \"gflops\",\n",
    "                \"h2d_ms\", \"compute_ms\", \"d2h_ms\",\n",
    "                \"power_w\", \"energy_j\", \"compute_energy_j\",\n",
    "            ])\n",
    "\n",
    "        power_w = POWER_W[device]\n",
    "        writer.writerow([\n",
    "            device, size_tag, n,\n",
    "            stats[\"mean_ms\"], stats[\"p50_ms\"], stats[\"p95_ms\"],\n",
    "            stats[\"gflops\"],\n",
    "            stats[\"h2d_ms\"], stats[\"compute_ms\"], stats[\"d2h_ms\"],\n",
    "            power_w, stats[\"energy_j\"], stats[\"compute_energy_j\"],\n",
    "        ])\n",
    "\n",
    "\n",
    "# =========================\n",
    "# Execution principale\n",
    "# =========================\n",
    "\n",
    "def main():\n",
    "    info = device_info()\n",
    "    print(\"Infos:\", info)\n",
    "    print(\"Debut des tests de multiplication matricielle\\n\")\n",
    "    print(\"\\nOn collecte: taille N, mean_ms, p50_ms, p95_ms, gflops.\")\n",
    "    print(\"GPU: h2d_ms, compute_ms, d2h_ms pour decomposer le temps.\")\n",
    "    print(\"Energie: power_w * temps (renseigner POWER_W).\\n\")\n",
    "\n",
    "    for size_tag, n in SIZES.items():\n",
    "        print(f\"\\n=== Taille {size_tag} (N={n}) ===\")\n",
    "\n",
    "        # CPU\n",
    "        cpu_stats = bench_cpu(n)\n",
    "        print(\"CPU:\", cpu_stats)\n",
    "        save_row(\"cpu\", size_tag, n, cpu_stats)\n",
    "\n",
    "        # GPU\n",
    "        if torch.cuda.is_available():\n",
    "            gpu_stats = bench_gpu(n)\n",
    "            print(\"GPU:\", gpu_stats)\n",
    "            save_row(\"gpu\", size_tag, n, gpu_stats)\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7ff05529",
   "metadata": {},
   "source": [
    "Les résultats du test MatMul montrent que le GPU (RTX 3060) n’apporte qu’un gain limité pour les petites matrices (S) en raison du coût des transferts mémoire et de la sous-utilisation de celui-ci.\n",
    "\n",
    "\n",
    "En revanche, pour des tailles moyennes et grandes (M et L), le GPU atteint un débit d’environ 3,2 TFLOPS, soit un facteur d’accélération proche de ×6 par rapport au CPU.\n",
    "\n",
    "\n",
    "Résultats principaux\n",
    "\n",
    "Petites matrices (1024) :\n",
    "le GPU est seulement légèrement plus rapide, car les transferts mémoire dominent.\n",
    "--> Le CPU reste pertinent pour de petites charges.\n",
    "\n",
    "Matrices moyennes (4096) :\n",
    "le GPU devient ≈ 4× plus rapide.\n",
    "--> Le parallélisme GPU commence à être pleinement exploité.\n",
    "\n",
    "Grandes matrices (8192) :\n",
    "le GPU atteint --> 10× la performance du CPU tout en consommant beaucoup moins d’énergie,\n",
    "même si ~40 % du temps reste lié aux transferts mémoire.\n",
    "\n",
    "\n",
    "### Points clés à retenir\n",
    "\n",
    "Le CPU est adapté aux petites tâches ou aux traitements séquentiels.\n",
    "\n",
    "Le GPU devient dominant lorsque l’intensité de calcul augmente.\n",
    "\n",
    "Les transferts mémoire constituent un goulot d’étranglement réel.\n",
    "\n",
    "Les workloads d’IA massifs privilégient donc les GPU ou accélérateurs spécialisés."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "torch-env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
