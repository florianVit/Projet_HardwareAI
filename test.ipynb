{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "74b58b71",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Torch version : 2.5.1+cu121\n",
      "CUDA dispo : True\n",
      "GPU : NVIDIA GeForce RTX 3060 Laptop GPU\n",
      "CPU ok : torch.Size([1000, 1000])\n",
      "GPU ok : torch.Size([1000, 1000])\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "print(\"Torch version :\", torch.__version__)\n",
    "print(\"CUDA dispo :\", torch.cuda.is_available())\n",
    "\n",
    "if torch.cuda.is_available():\n",
    "    print(\"GPU :\", torch.cuda.get_device_name(0))\n",
    "\n",
    "x = torch.rand(1000, 1000)\n",
    "y = torch.mm(x, x)\n",
    "\n",
    "print(\"CPU ok :\", y.shape)\n",
    "\n",
    "if torch.cuda.is_available():\n",
    "    x = x.to(\"cuda\")\n",
    "    y = torch.mm(x, x)\n",
    "    print(\"GPU ok :\", y.shape)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f2d534da",
   "metadata": {},
   "source": [
    "### Si tout marche bien coté installation/fonctionnement du matos on commence\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "368b0340",
   "metadata": {},
   "source": [
    "# Bench_matmul (CPU vs GPU)\n",
    "le TPU se fait sur collab"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "b8be518b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Début des tests de multiplication matricielle\n",
      "\n",
      "Ici on va avoir : - la taille de la matrice (N) \n",
      "- le temps moyen d'exécution (mean_ms) \n",
      "- les percentiles 50 et 95 (p50_ms, p95_ms)(par exemple p50 représente le temps typique d'exécution (50% rapide, 50% lent) ) \n",
      "- les GFLOPS calculés (gflops) \n",
      "- pour le GPU, on aura aussi les temps de transfert Host to Device (h2d_ms), de calcul (compute_ms) et Device to Host (d2h_ms)\n",
      "\n",
      "\n",
      "=== Taille S (N=1024) ===\n",
      "CPU: {'mean_ms': 4.463385000008202, 'p50_ms': 4.8543000002609915, 'p95_ms': 5.240304999369982, 'gflops': 481.1334106280444, 'h2d_ms': 0, 'compute_ms': 4.463385000008202, 'd2h_ms': 0}\n",
      "GPU: {'mean_ms': 3.679195997071365, 'p50_ms': 2.396276004426909, 'p95_ms': 10.633936304532833, 'gflops': 583.6828616114484, 'h2d_ms': 1.5788440000414994, 'compute_ms': 1.21033599704504, 'd2h_ms': 0.8900159999848256}\n",
      "\n",
      "=== Taille M (N=4096) ===\n",
      "CPU: {'mean_ms': 276.1603730000297, 'p50_ms': 271.9075000004523, 'p95_ms': 328.8107949997084, 'gflops': 497.67804112860614, 'h2d_ms': 0, 'compute_ms': 276.1603730000297, 'd2h_ms': 0}\n",
      "GPU: {'mean_ms': 48.59870415262944, 'p50_ms': 48.40296816699629, 'p95_ms': 53.05582323529888, 'gflops': 2828.037411046151, 'h2d_ms': 18.35001199994622, 'compute_ms': 17.93059715270996, 'd2h_ms': 12.318094999973255}\n",
      "\n",
      "=== Taille L (N=8192) ===\n",
      "CPU: {'mean_ms': 1938.8983810000352, 'p50_ms': 1918.483450000167, 'p95_ms': 2134.9395849999382, 'gflops': 567.0805848055323, 'h2d_ms': 0, 'compute_ms': 1938.8983810000352, 'd2h_ms': 0}\n",
      "GPU: {'mean_ms': 292.36991245267023, 'p50_ms': 289.6299959601929, 'p95_ms': 309.5222062698511, 'gflops': 3760.6866539455987, 'h2d_ms': 74.47635500005163, 'compute_ms': 159.13742645263673, 'd2h_ms': 58.756130999981906}\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import time\n",
    "import numpy as np\n",
    "import csv\n",
    "from pathlib import Path\n",
    "\n",
    "# =========================\n",
    "# Configuration générale\n",
    "# =========================\n",
    "\n",
    "SIZES = { #on def les tailles pour tester\n",
    "    \"S\": 1024,\n",
    "    \"M\": 4096,\n",
    "    \"L\": 8192,  #changer selon votre RAM et GPU, comme j'ai mit sur discord, je suis à 16GB de RAM et j'ai une RTX 3060\n",
    "}\n",
    "\n",
    "WARMUP = 20 # nombre d'itérations de warmup pour \"chauffer\" le cache et les optimisations\n",
    "ITERS = 100 \n",
    "DTYPE = torch.float32\n",
    "\n",
    "RESULTS_DIR = Path(\"results\")\n",
    "RESULTS_DIR.mkdir(exist_ok=True)\n",
    "\n",
    "CSV_PATH = RESULTS_DIR / \"matmul_cpu_gpu.csv\"\n",
    "\n",
    "\n",
    "# =========================\n",
    "# Outils statistiques\n",
    "# =========================\n",
    "\n",
    "def compute_stats(times_ms):\n",
    "    times = np.array(times_ms)\n",
    "    return {\n",
    "        \"mean_ms\": times.mean(),\n",
    "        \"p50_ms\": np.percentile(times, 50),\n",
    "        \"p95_ms\": np.percentile(times, 95),\n",
    "    }\n",
    "\n",
    "\n",
    "def gflops(n, time_ms):\n",
    "    flops = 2 * (n ** 3)\n",
    "    return flops / (time_ms / 1000) / 1e9\n",
    "\n",
    "\n",
    "# =========================\n",
    "# Benchmark CPU\n",
    "# =========================\n",
    "\n",
    "def bench_cpu(n):\n",
    "    x = torch.rand((n, n), dtype=DTYPE)\n",
    "    y = torch.rand((n, n), dtype=DTYPE)\n",
    "\n",
    "    # Warmup\n",
    "    for _ in range(WARMUP):\n",
    "        torch.mm(x, y)\n",
    "\n",
    "    times = []\n",
    "\n",
    "    for _ in range(ITERS):\n",
    "        start = time.perf_counter()\n",
    "        torch.mm(x, y)\n",
    "        end = time.perf_counter()\n",
    "\n",
    "        times.append((end - start) * 1000)\n",
    "\n",
    "    stats = compute_stats(times)\n",
    "    stats[\"gflops\"] = gflops(n, stats[\"mean_ms\"])\n",
    "    stats[\"h2d_ms\"] = 0\n",
    "    stats[\"compute_ms\"] = stats[\"mean_ms\"]\n",
    "    stats[\"d2h_ms\"] = 0\n",
    "\n",
    "    return stats\n",
    "\n",
    "\n",
    "# =========================\n",
    "# Benchmark GPU\n",
    "# =========================\n",
    "\n",
    "def bench_gpu(n):\n",
    "    device = \"cuda\"\n",
    "\n",
    "    x_cpu = torch.rand((n, n), dtype=DTYPE)\n",
    "    y_cpu = torch.rand((n, n), dtype=DTYPE)\n",
    "\n",
    "    # Warmup complet\n",
    "    for _ in range(WARMUP):\n",
    "        x = x_cpu.to(device)\n",
    "        y = y_cpu.to(device)\n",
    "        torch.cuda.synchronize()\n",
    "        torch.mm(x, y)\n",
    "        torch.cuda.synchronize()\n",
    "\n",
    "    times = []\n",
    "    h2d_list = []\n",
    "    compute_list = []\n",
    "    d2h_list = []\n",
    "\n",
    "    for _ in range(ITERS):\n",
    "        # H → D\n",
    "        t0 = time.perf_counter()\n",
    "        x = x_cpu.to(device)\n",
    "        y = y_cpu.to(device)\n",
    "        torch.cuda.synchronize()\n",
    "        h2d = (time.perf_counter() - t0) * 1000\n",
    "\n",
    "        # Compute (CUDA events)\n",
    "        start = torch.cuda.Event(enable_timing=True)\n",
    "        end = torch.cuda.Event(enable_timing=True)\n",
    "\n",
    "        start.record()\n",
    "        out = torch.mm(x, y)\n",
    "        end.record()\n",
    "\n",
    "        torch.cuda.synchronize()\n",
    "        compute = start.elapsed_time(end)\n",
    "\n",
    "        # D → H\n",
    "        t0 = time.perf_counter()\n",
    "        out.cpu()\n",
    "        torch.cuda.synchronize()\n",
    "        d2h = (time.perf_counter() - t0) * 1000\n",
    "\n",
    "        total = h2d + compute + d2h\n",
    "\n",
    "        times.append(total)\n",
    "        h2d_list.append(h2d)\n",
    "        compute_list.append(compute)\n",
    "        d2h_list.append(d2h)\n",
    "\n",
    "    stats = compute_stats(times)\n",
    "    stats[\"gflops\"] = gflops(n, stats[\"mean_ms\"])\n",
    "    stats[\"h2d_ms\"] = np.mean(h2d_list)\n",
    "    stats[\"compute_ms\"] = np.mean(compute_list)\n",
    "    stats[\"d2h_ms\"] = np.mean(d2h_list)\n",
    "\n",
    "    return stats\n",
    "\n",
    "\n",
    "# =========================\n",
    "# Sauvegarde CSV\n",
    "# =========================\n",
    "\n",
    "def save_row(device, size_tag, n, stats):\n",
    "    file_exists = CSV_PATH.exists()\n",
    "\n",
    "    with open(CSV_PATH, \"a\", newline=\"\") as f:\n",
    "        writer = csv.writer(f)\n",
    "\n",
    "        if not file_exists:\n",
    "            writer.writerow([\n",
    "                \"device\", \"size\", \"N\",\n",
    "                \"mean_ms\", \"p50_ms\", \"p95_ms\",\n",
    "                \"gflops\",\n",
    "                \"h2d_ms\", \"compute_ms\", \"d2h_ms\"\n",
    "            ])\n",
    "\n",
    "        writer.writerow([\n",
    "            device, size_tag, n,\n",
    "            stats[\"mean_ms\"], stats[\"p50_ms\"], stats[\"p95_ms\"],\n",
    "            stats[\"gflops\"],\n",
    "            stats[\"h2d_ms\"], stats[\"compute_ms\"], stats[\"d2h_ms\"]\n",
    "        ])\n",
    "\n",
    "\n",
    "# =========================\n",
    "# Exécution principale\n",
    "# =========================\n",
    "\n",
    "def main():\n",
    "    print(\"Début des tests de multiplication matricielle\\n\")\n",
    "    print(\"\\n Ici on va avoir : - la taille de la matrice (N) \\n- le temps moyen d'exécution (mean_ms) \\n- les percentiles 50 et 95 (p50_ms, p95_ms)(par exemple p50 représente le temps typique d'exécution (50% rapide, 50% lent) )\"\n",
    "          \" \\n- les GFLOPS calculés (gflops) \\n- pour le GPU, on aura aussi les temps de transfert Host to Device (h2d_ms), de calcul (compute_ms) et Device to Host (d2h_ms)\\n\")\n",
    "\n",
    "    for size_tag, n in SIZES.items():\n",
    "        print(f\"\\n=== Taille {size_tag} (N={n}) ===\")\n",
    "\n",
    "        # CPU\n",
    "        cpu_stats = bench_cpu(n)\n",
    "        print(\"CPU:\", cpu_stats)\n",
    "        save_row(\"cpu\", size_tag, n, cpu_stats)\n",
    "\n",
    "        # GPU\n",
    "        if torch.cuda.is_available():\n",
    "            gpu_stats = bench_gpu(n)\n",
    "            print(\"GPU:\", gpu_stats)\n",
    "            save_row(\"gpu\", size_tag, n, gpu_stats)\n",
    "\n",
    "    \n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7ff05529",
   "metadata": {},
   "source": [
    "Les résultats du test MatMul montrent que le GPU RTX 3060 n’apporte qu’un gain limité pour les petites matrices (S) en raison du coût des transferts mémoire et de la sous-utilisation de celui-ci.\n",
    "\n",
    "\n",
    "En revanche, pour des tailles moyennes et grandes, le GPU atteint un débit d’environ 3,2 TFLOPS, soit un facteur d’accélération proche de ×6 par rapport au CPU.\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "torch-env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
